---
title: "Adult Learning, Welsh Language & the Data Science Accelerator Programme"
author: "BJCollier & SRLloyd"
date: "31 May 2018"
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data08")
```

## Project and Project Aims

As a result of the Welsh Language (Wales) Measure 2011, Welsh has official status in Wales sitting within a legislative framework which places a duty on local authorities, among other bodies, to promote the Welsh Language. Pembrokeshire County Council (PCC) is committed to the principle of promoting and facilitating the use of the Welsh language; consequently the Authority provides Welsh Language courses for both its employees and the public. This aligns with the Welsh Government strategy of reaching a million Welsh speakers by 2050 and to have 1000 new adult Welsh speakers every year until the next census in 2021 <https://gov.wales/topics/welshlanguage/welsh-language-strategy-and-policies/cymraeg-2050-welsh-language-strategy/?lang=en>.

The project aims to analyse the data held by PCC on Welsh adult learning courses to identify possible steps to enable the further growth of the language. 9 year's worth of anonymised data was made available to the project by the Learning Pembrokeshire team in conjunction with Swansea University.

The secondary aim of the project is to provide 2 PCC employees with the skills used in modern data science to improve the efficiency of data analysis within the authority. These skills learnt under the tutelage of data science experts at the Office of National Statistics could then be dissseminated throughout the organisation.

With both aims in mind the project is run exclusively in the programming language R. This makes analysis reproducible, scalable and accessible, R being an open source project with a wealth of community guidance. It is our firm belief that the authority should look to upskill employees in the use of R as part of its transformation drive.

### Data Processing

It is commonly said that 80% of a data science project is spent processing the data. The adult learning data made available to the project is a good example of a non-synthetic dataset, essentially a selection of linked tables from a database subset for each academic year. First step - import the data!

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
#defining header lists for the files to be read in
#'enrolment', 'location', 'module', 'student', 'tutormodule' and from 2012 onwards, 'entry'
header_en <- list("provider_code", 
                      "student_id", 
                      "module_id", 
                      "enrolment_date", 
                      "enrolment_status",
                      "employer_role",
                      "employment_status",
                      "fee_code",
                      "fee_amount",
                      "instalments",
                      "withdrawal_date",
                      "withdrawal_reason",
                      "withdrawal_code",
                      "mode_study"
                      )

header_en1 <- list("provider_code", 
                  "student_id", 
                  "module_id", 
                  "enrolment_date", 
                  "enrolment_status",
                  "employer_role",
                  "employment_status",
                  "fee_code",
                  "fee_amount",
                  "instalments",
                  "withdrawal_date",
                  "withdrawal_reason",
                  "withdrawal_code",
                  "mode_study",
                  "tt_postcode",
                  "tt_accommodation"
)

header_lo <- list("provider_code",
                  "location_code",
                  "location_name",
                  "location_type",
                  "location_snam",
                  "add1",
                  "add2",
                  "add3",
                  "add4",
                  "postcode",
                  "telephone",
                  "local_authority"
                  )

header_tumo <- list("provider_code",
                "module_id",
                "tutor_id"
                )

header_stu <- list("provider_code",
                   "student_id",
                   "husid",
                   "title",
                   "surname",
                   "name1",
                   "name2",
                   "name3",
                   "known_as",
                   "surname_at_16",
                   "gender",
                   "dob",
                   "welsh_speaker",
                   "disability",
                   "ethnicity",
                   "domicile",
                   "natid1",
                   "natid2",
                   "nationality",
                   "highest_qual",
                   "disability_allowance",
                   "uln",
                   "hwebsite",
                   "hbrochure",
                   "hflyer",
                   "hadvert",
                   "hvenue",
                   "htutor",
                   "hfriend",
                   "do_not_contact",
                   "deceased",
                   "email",
                   "add1",
                   "add2",
                   "add3",
                   "add4",
                   "postcode"
                   )

header_stu1 <- list("provider_code",
                   "student_id",
                   "husid",
                   "title",
                   "surname",
                   "name1",
                   "name2",
                   "name3",
                   "known_as",
                   "surname_at_16",
                   "gender",
                   "dob",
                   "welsh_speaker",
                   "disability",
                   "ethnicity",
                   "domicile",
                   "natid1",
                   "natid2",
                   "nationality",
                   "highest_qual",
                   "disability_allowance",
                   "uln",
                   "hwebsite",
                   "hbrochure",
                   "hflyer",
                   "hadvert",
                   "hvenue",
                   "htutor",
                   "hfriend",
                   "do_not_contact",
                   "deceased",
                   "email",
                   "add1",
                   "add2",
                   "add3",
                   "add4",
                   "postcode",
                   "tel_day",
                   "tel_eve",
                   "mobile"
)

header_stu2 <- list("provider_code",
                    "student_id",
                    "husid",
                    "title",
                    "surname",
                    "name1",
                    "name2",
                    "name3",
                    "known_as",
                    "surname_at_16",
                    "gender",
                    "dob",
                    "welsh_speaker",
                    "disability",
                    "ethnicity",
                    "domicile",
                    "natid1",
                    "natid2",
                    "nationality",
                    "highest_qual",
                    "disability_allowance",
                    "uln",
                    "hwebsite",
                    "hbrochure",
                    "hflyer",
                    "hadvert",
                    "hvenue",
                    "htutor",
                    "hfriend",
                    "do_not_contact",
                    "deceased",
                    "email",
                    "add1",
                    "add2",
                    "add3",
                    "add4",
                    "postcode",
                    "tel_day",
                    "tel_eve",
                    "mobile",
                    "kin_surname",
                    "kin_name1",
                    "kin_name2",
                    "kin_name3",
                    "kin_add1",
                    "kin_add2",
                    "kin_add3",
                    "kin_add4",
                    "kin_postode",
                    "kin_tel_day",
                    "kin_tel_eve",
                    "kin_mob",
                    "kin_relationship",
                    "emergency_contact"
)

header_mo <- list("provider_code",
                  "module_id",
                  "module_title",
                  "contact_hours",
                  "max_students",
                  "min_students",
                  "fe_qualaim",
                  "module_mode_study",
                  "unit_length",
                  "length",
                  "location_code",
                  "module_start_date",
                  "module_end_date",
                  "module_start_time",
                  "module_end_time",
                  "cancelled_date",
                  "abandoned_date",
                  "taught_mon",
                  "taught_tue",
                  "taught_wed",
                  "taught_thu",
                  "taught_fri",
                  "taught_sat",
                  "taught_sun"
                  )

header_entry <- list("provider_code",
                     "student_id",
                     "module_id",
                     "highest_qual",
                     "domicile",
                     "previous_inst",
                     "year_left_inst",
                     "degree_where",
                     "degree_when",
                     "postcode")
```
The following code contains a vital component of programming in R - a function and a loop
```{r echo=TRUE, message=FALSE, warning=FALSE}
filenames <- list.files(path = ".",pattern = ".CSV")
#this loops the files and names them file1, file2 etc - note header = FALSE and na.strings
for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}
#header = FALSE leaves space for us to assign the header titles from our header list
colnames(file1) <- header_en
colnames(file2) <- header_lo
colnames(file3) <- header_mo
colnames(file4) <- header_stu
colnames(file5) <- header_tumo
#files named with year reference
en08 <- file1 %>% mutate(year = 2008) %>% select(year, everything())
lo08 <- file2 %>% mutate(year = 2008) %>% select(year, everything())
mo08 <- file3 %>% mutate(year = 2008) %>% select(year, everything())
stu08 <- file4 %>% mutate(year = 2008) %>% select(year, everything())
tumo08 <- file5 %>% mutate(year = 2008) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data09")
```
Which imports the files for one year - this code is simply copied and pasted changing just the file path and the reference year for the remaining 8 years
```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2009
filenames <- list.files(path = ".",pattern = ".CSV")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en
colnames(file2) <- header_lo
colnames(file3) <- header_mo
colnames(file4) <- header_stu
colnames(file5) <- header_tumo
en09 <- file1 %>% mutate(year = 2009) %>% select(year, everything())
lo09 <- file2 %>% mutate(year = 2009) %>% select(year, everything())
mo09 <- file3 %>% mutate(year = 2009) %>% select(year, everything())
stu09 <- file4 %>% mutate(year = 2009) %>% select(year, everything())
tumo09 <- file5 %>% mutate(year = 2009) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data10")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2010
filenames <- list.files(path = ".",pattern = ".CSV")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en
colnames(file2) <- header_lo
colnames(file3) <- header_mo
colnames(file4) <- header_stu
colnames(file5) <- header_tumo
en10 <- file1 %>% mutate(year = 2010) %>% select(year, everything())
lo10 <- file2 %>% mutate(year = 2010) %>% select(year, everything())
mo10 <- file3 %>% mutate(year = 2010) %>% select(year, everything())
stu10 <- file4 %>% mutate(year = 2010) %>% select(year, everything())
tumo10 <- file5 %>% mutate(year = 2010) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data11")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2011
filenames <- list.files(path = ".",pattern = ".csv")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en
colnames(file2) <- header_lo
colnames(file3) <- header_mo
colnames(file4) <- header_stu
colnames(file5) <- header_tumo
en11 <- file1 %>% mutate(year = 2011) %>% select(year, everything())
lo11 <- file2 %>% mutate(year = 2011) %>% select(year, everything())
mo11 <- file3 %>% mutate(year = 2011) %>% select(year, everything())
stu11 <- file4 %>% mutate(year = 2011) %>% select(year, everything())
tumo11 <- file5 %>% mutate(year = 2011) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data12")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2012
filenames <- list.files(path = ".",pattern = ".csv")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en1
colnames(file2) <- header_entry
colnames(file3) <- header_lo
colnames(file4) <- header_mo
colnames(file5) <- header_stu1
colnames(file6) <- header_tumo
en12 <- file1 %>% mutate(year = 2012) %>% select(year, everything())
entry12 <- file2 %>% mutate(year = 2012) %>% select(year, everything())
lo12 <- file3 %>% mutate(year = 2012) %>% select(year, everything())
mo12 <- file4 %>% mutate(year = 2012) %>% select(year, everything())
stu12 <- file5 %>% mutate(year = 2012) %>% select(year, everything())
tumo12 <- file6 %>% mutate(year = 2012) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data13")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2013
filenames <- list.files(path = ".",pattern = ".csv")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en1
colnames(file2) <- header_entry
colnames(file3) <- header_lo
colnames(file4) <- header_mo
colnames(file5) <- header_stu1
colnames(file6) <- header_tumo
en13 <- file1 %>% mutate(year = 2013) %>% select(year, everything())
entry13 <- file2 %>% mutate(year = 2013) %>% select(year, everything())
lo13 <- file3 %>% mutate(year = 2013) %>% select(year, everything())
mo13 <- file4 %>% mutate(year = 2013) %>% select(year, everything())
stu13 <- file5 %>% mutate(year = 2013) %>% select(year, everything())
tumo13 <- file6 %>% mutate(year = 2013) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data14")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2014
filenames <- list.files(path = ".",pattern = ".csv")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en1
colnames(file2) <- header_entry
colnames(file3) <- header_lo
colnames(file4) <- header_mo
colnames(file5) <- header_stu1
colnames(file6) <- header_tumo
en14 <- file1 %>% mutate(year = 2014) %>% select(year, everything())
entry14 <- file2 %>% mutate(year = 2014) %>% select(year, everything())
lo14 <- file3 %>% mutate(year = 2014) %>% select(year, everything())
mo14 <- file4 %>% mutate(year = 2014) %>% select(year, everything())
stu14 <- file5 %>% mutate(year = 2014) %>% select(year, everything())
tumo14 <- file6 %>% mutate(year = 2014) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data15")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2015
filenames <- list.files(path = ".",pattern = ".CSV")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en1
colnames(file2) <- header_entry
colnames(file3) <- header_lo
colnames(file4) <- header_mo
colnames(file5) <- header_stu
colnames(file6) <- header_tumo
en15 <- file1 %>% mutate(year = 2015) %>% select(year, everything())
entry15 <- file2 %>% mutate(year = 2015) %>% select(year, everything())
lo15 <- file3 %>% mutate(year = 2015) %>% select(year, everything())
mo15 <- file4 %>% mutate(year = 2015) %>% select(year, everything())
stu15 <- file5 %>% mutate(year = 2015) %>% select(year, everything())
tumo15 <- file6 %>% mutate(year = 2015) %>% select(year, everything())
knitr::opts_knit$set(root.dir = "./Data - Swansea University contract/data16")
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#YEAR 2016
filenames <- list.files(path = ".",pattern = ".CSV")

for (i in 1:length(filenames)){
  oname = paste("file", i, sep = "")
  assign(oname, read.csv(filenames[i],header = FALSE,na.strings = ""))#(paste(oname, ".CSV", sep = "")))
}

colnames(file1) <- header_en1
colnames(file2) <- header_entry
colnames(file3) <- header_lo
colnames(file4) <- header_mo
colnames(file5) <- header_stu
colnames(file6) <- header_tumo
en16 <- file1 %>% mutate(year = 2016) %>% select(year, everything())
entry16 <- file2 %>% mutate(year = 2016) %>% select(year, everything())
lo16 <- file3 %>% mutate(year = 2016) %>% select(year, everything())
mo16 <- file4 %>% mutate(year = 2016) %>% select(year, everything())
stu16 <- file5 %>% mutate(year = 2016) %>% select(year, everything())
tumo16 <- file6 %>% mutate(year = 2016) %>% select(year, everything())

#remove unnecessary columns in files with differing variables ready for rbind
en12 <- select(en12, year:mode_study)
en13 <- select(en13, year:mode_study)
en14 <- select(en14, year:mode_study)
en15 <- select(en15, year:mode_study)
en16 <- select(en16, year:mode_study)
stu12 <- select(stu12, year:postcode)
stu13 <- select(stu13, year:postcode)
stu14 <- select(stu14, year:postcode)
knitr::opts_knit$set(root.dir = ".")
```
This results in approximately 50 dataframes (or data tables) that need to be combined. The following code shows the steps used - from simply appending dataframes to joining on a reference point
```{r echo=TRUE, message=FALSE, warning=FALSE}
#bind rows to create all years data frames
all_en <- bind_rows(en08, en09, en10, en11, en12, en13, en14, en15, en16)
all_entry <- bind_rows(entry12, entry13, entry14, entry15, entry16)
all_lo <- bind_rows(lo08, lo09, lo10, lo11, lo12, lo13, lo14, lo15, lo16)
all_mo <- bind_rows(mo08, mo09, mo10, mo11, mo12, mo13, mo14, mo15, mo16)
all_stu <- bind_rows(stu08, stu09, stu10, stu11, stu12, stu13, stu14, stu15, stu16)
all_tumo <- bind_rows(tumo08, tumo09, tumo10, tumo11, tumo12, tumo13, tumo14, tumo15, tumo16)
#provide unique identifier for merging data frames
all_en <- all_en %>% mutate(combi = paste0(year, student_id, module_id))
all_entry <- all_entry %>% mutate(combi = paste0(year, student_id, module_id))
#merge enrolment and entry
all_ent <- all_en %>% left_join(all_entry, by = "combi")
#unique identifier for bringing in student info
all_ent <- all_ent %>% mutate(combi2 = paste0(year.x, student_id.x))
all_stu <- all_stu %>% mutate(combi2 = paste0(year, student_id))
#merge enrolment and student
all_sid <- all_ent %>% left_join(all_stu, by = "combi2")
#remove na
all_sid <- all_sid %>% select_if(~sum(!is.na(.)) > 0)
#now we need to do the same to the module, location and tutor information...
#make the unique id to combine on
all_mo <- all_mo %>% mutate(combi3 = paste0(year, location_code))
all_lo <- all_lo %>% mutate(combi3 = paste0(year, location_code))
#merge module and location
all_molo <- all_mo %>% left_join(all_lo, by = "combi3")
#unique identifier for bringing in tutor info
all_molo <- all_molo %>% mutate(combi4 = paste0(year.x, module_id))
all_tumo <- all_tumo %>% mutate(combi4 = paste0(year, module_id))
#merge module and tutor
all_molotumo <- all_molo %>% left_join(all_tumo, by = "combi4")
#ultimately we will get the information from the modular file into the student file
#producing one file with all valid variables before linking to outcomes
#therefore, as before, we need an identifier to merge on
all_sid <- all_sid %>% mutate(combi4 = paste0(year.x, module_id.x))
#this identifier (combi4) is already in molotumo so we can merge the two files
stu.rol <- all_sid %>% left_join(all_molotumo, by = "combi4")
#remove NA filled columns
stu.rol <- stu.rol %>% select_if(~sum(!is.na(.)) > 0)
#remove surperflous columns
student.enrolment <- stu.rol %>% select(-ends_with("y.x"), -ends_with("x.y"), -ends_with("y.y"),
                               -ends_with("x.x.x"), -ends_with("y.y.y"))
#what is the structural make-up of our combined dataframe
#str(student.enrolment)
```
Data importation complete! But now various problems become apparent. What are the variables defined as? What is the meaning of some of the coded variables? Are date references in the correct format? And postcodes? Are there any anomalies? Or missing data?
Where obvious, the problems outlined above were rectified with the understanding that more issues may become apparent as the analysis develops.
```{r echo=TRUE, message=FALSE, warning=FALSE}
#we're going to have a look at reducing variables
#before doing some EDA
sv <- select(student.enrolment, c(1, 5:6, 8, 10, 12:14, 18, 24:28, 30, 33, 43:44, 46:47, 51, 57:58, 60:65, 71:72, 74))
#str(sv)
#we need to 'unite' out highest qual columns - one contains NA up til 2011, the other NA beyond 2011
sv <- unite(sv, highest_qual, highest_qual.x, highest_qual.y, sep = "", remove = TRUE)
#the united column contains NA's, we remove these using 'gsub'
sv$highest_qual <- gsub("NA", "", sv$highest_qual)
#additionally this column needs tidying - there are two sets of coded quals, pre and post 2011
unique(sv$highest_qual)
sv$highest_qual <- gsub("H80", "Degree/ProfQual", sv$highest_qual)
sv$highest_qual <- gsub("C80", "Degree/ProfQual", sv$highest_qual)
sv$highest_qual <- gsub("28", "Degree/ProfQual", sv$highest_qual)
sv$highest_qual <- gsub("P80", "Level3", sv$highest_qual)
sv$highest_qual <- gsub("40", "Level3", sv$highest_qual)
sv$highest_qual <- gsub("Q80", "Level2", sv$highest_qual)
sv$highest_qual <- gsub("55", "Level2", sv$highest_qual)
sv$highest_qual <- gsub("R80", "Level1", sv$highest_qual)
sv$highest_qual <- gsub("56", "Level1", sv$highest_qual)
sv$highest_qual <- gsub("X06", "Unknown", sv$highest_qual)
sv$highest_qual <- gsub("99", "Unknown", sv$highest_qual)
unique(sv$highest_qual)
#....and welsh speaker
sv$welsh_speaker <- gsub("1", "fluent", sv$welsh_speaker)
sv$welsh_speaker <- gsub("2", "non-fluent", sv$welsh_speaker)
sv$welsh_speaker <- gsub("3", "non-speaker", sv$welsh_speaker)
sv$welsh_speaker <- gsub("9", "unknown", sv$welsh_speaker)
#employment status would be good too but the coding doesn't match the guidance
#coding received at last....
sv$employment_status <- gsub("1", "employedFT", sv$employment_status)
sv$employment_status <- gsub("3", "employedPT", sv$employment_status)
sv$employment_status <- gsub("11", "student", sv$employment_status)
sv$employment_status <- gsub("15", "self-employed", sv$employment_status)
sv$employment_status <- gsub("16", "other", sv$employment_status)
sv$employment_status <- gsub("18", "unemployed", sv$employment_status)
sv$employment_status <- gsub("19", "employedLE", sv$employment_status)
sv$employment_status <- gsub("21", "retired", sv$employment_status)
#rename columns
sv <- rename(sv, year = year.x.x)
sv <- rename(sv, centre_postcode = postcode)
sv <- rename(sv, postcode = postcode.y)
#clean global environment with 'gdata' and 'keep'
library(gdata)
keep(accreditation, student.enrolment, sv, sure = TRUE)
#make dates into dates and determine age at enrolment from enrolment date and dob
library(lubridate)
sv$enrolment_date <- dmy(sv$enrolment_date)
#lubridate has a problem with abbreviated year assuming it to be 2000+
#the following line introduces 19 - we can safely assume all adult learners in the
#dataset were born before 2000
sv$dob <- as.Date(format(as.Date(sv$dob, format = "%d-%b-%y"), "19%y%m%d"), "%Y%m%d")
#age determination
sv <- sv %>% mutate(age = (dob %--% enrolment_date) %/% years(1))
#what's the basic make-up of our adult learners
summary(sv$age)
sv %>% count(gender)
#enrolment_status is our dependent variable - we need to factorise it to be 1 and 0
#enrolled, pre-enrolled and transferred can all be classed as 1, withdrawn then 0
#we can create things in r and then assign them to our data frame
# Make an empty list/vector
empty_col <- c()
#Make a vector of the levels we want to assign the same value to (1 in this case)
crit_enrlmt <- c("ENROLLED", "TRANSFERRED", "PRE-ENROLLED")
#then write our if_else loop
for (i in 1:length(sv$enrolment_status)){ 
  if (sv$enrolment_status[i] %in% crit_enrlmt){
    empty_col[i] <- 1
  } 
  else { 
    empty_col[i] <- 0
  } 
}   
#Assign a name to the empty_col vector and add it to the 'sv' dataframe
sv$numeric_en <- empty_col
#we need to turn some of our character strings to factors
sv$enrolment_status <- as.factor(sv$enrolment_status)
sv$employment_status <- as.factor(sv$employment_status)
sv$withdrawal_reason <- as.factor(sv$withdrawal_reason)
sv$withdrawal_code <- as.factor(sv$withdrawal_code)
sv$highest_qual <- as.factor(sv$highest_qual)
sv$welsh_speaker <- as.factor(sv$welsh_speaker)
sv$disability <- as.factor(sv$disability)
sv$ethnicity <- as.factor(sv$ethnicity)
sv$deceased <- as.factor(sv$deceased)
sv$module_mode_study <- as.factor(sv$module_mode_study)
sv$taught_mon <- as.factor(sv$taught_mon)
sv$taught_tue <- as.factor(sv$taught_tue)
sv$taught_wed <- as.factor(sv$taught_wed)
sv$taught_thu <- as.factor(sv$taught_thu)
sv$taught_fri <- as.factor(sv$taught_fri)
sv$taught_sat <- as.factor(sv$taught_sat)
sv$tutor_id <- as.factor(sv$tutor_id)
sv$numeric_en <- as.factor(sv$numeric_en)
#...take a look 
#str(sv) 
```
### Supervised Machine Learning and Exploratory Data Analysis

But the project was eager to dive into where R comes into its own - namely machine learning and visualisation. One of the initial thoughts to the project was to determine the likelihood of someone passing a Welsh learning course. It quickly became apparent that the vast majority of people completing courses pass, so instead the project set about determining the likelihood of someone lasting the course. With enrolment status as one of the variables are there enough withdrawals to model on...
```{r echo=TRUE, message=FALSE, warning=FALSE}
table(sv$enrolment_status)
```
Yes! The project will utilise a multiple regression technique to predict the enrolment status based upon a combination of variables, both categorical and continuous. But first good model practice requires that the data is split into a training dataset and a testing dataset, which can be done at random with the following code.
```{r echo=TRUE, message=FALSE, warning=FALSE}
#select training and testing datasets taking a random sample of 75% for our train set
smp_size <- floor(0.75 * nrow(sv))
#set seed to make partition reproducible
set.seed(123)
#sample the dataset to get 75% at random
train_ind <- sample(seq_len(nrow(sv)), size = smp_size)
#make train and test set
train <- sv[train_ind, ]
test <- sv[-train_ind, ]
```
and after trial and error models, model 6 gives us the best error rate
```{r echo=TRUE, message=FALSE, warning=FALSE}
#make models using glm (for logistic regression), adding variables in as you go
model6 <- glm(numeric_en ~ age + gender + welsh_speaker + natid1 + contact_hours, data = train, family = binomial)
summary(model6)
plot(model6)
```
One problem with the ease of modelling in R is that it can become a bit like trial and error with an increasing number of variables added to find our lowest error rate but there is a risk to this process - overfitting. The essence of overfitting is to have unknowingly extracted some of the residual variation (ie the noise) as if that variation represented underlying model structure. So it benefits one to find out more about the variables within the data - what is predictive, and what isn't? One usual tool in exploratory data analysis is to use graphs to tell us things about our data...
```{r echo=TRUE, message=FALSE, warning=FALSE}
#what is the age distribution of our data, and what proportion withdraw from the course
#here 1 stands for enrolled and 0 for withdrawn
ggplot(sv, aes(age, colour = numeric_en))+
  geom_histogram()+
  ggtitle("Age Distribution with Enrolment Status")
```
For supervised modelling purposes it was necessary to use a binary interpretation of enrolled and withdrawn. Here and in the following 1 stands for enrolled and 0 for withdrawn in our numeric_en variable. This graph shows that we have more older people within the data and there seems to be some correlation between age and enrolment status. The perhaps busier age groups around middle age appear more likely to withdraw. We can view withdrawal and age relation with a boxplot to validate this...
```{r echo=TRUE, message=FALSE, warning=FALSE}
#boxplot
ggplot(sv, aes(numeric_en, age))+
  geom_boxplot()+
  ggtitle("Enrolment Status and Average Age")
```
The boxplot then shows the quartile information of our age distribution, the box representing the 25th to 75th percentiles with the black line through the middle representing the median. If we consider that older people on average are likely to have fewer time constraints, it seems logical that older people are less likely to withdraw.
We saw earlier that far more females exist within our dataset than males. Might males be more likely to withdraw?
```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(sv[!is.na(sv$gender),], aes(gender, fill = numeric_en))+
  geom_bar()+
  ggtitle("Gender Distribution")
```
but sometimes its hard to make out a sense of comparative proportionality on the numbers alone. This same graph can be represented as a percentage or proportion...
```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(sv[!is.na(sv$gender),], aes(gender, fill = numeric_en))+
  geom_bar(position = "fill")+
  ggtitle("Gender Enrolment Proportionality")
```
and we see that the withdrawal rate is more or less the same across the sexes. Another popular data science technique is to display a number of histograms representing different groups within the same chart axis. For example, what might a chart of highest qualification, course fee amount and enrolment status look like...
```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(sv, aes(fee_amount, fill = numeric_en))+
  geom_histogram(binwidth = 10)+
  coord_cartesian(xlim = c(0, 200))+
  facet_wrap(~highest_qual)+
  ggtitle("Withdrawals, Qualifications and Fees")
```
Sometimes charts like these help to confirm or debunk hypothesis. Instinctively, we might think that if you were well educated and the course is free, you're more likely to stay on the course. There is some evidence to suggest that the opposite may be true.
These graphs are all fairly innocuous - but we have the power to be a little confrontational. Are there community learning centres where people are more likely to stay on the course?
```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(sv, aes(centre_postcode, fill = numeric_en))+
  geom_bar(position = "fill")+
  coord_flip()+
  theme(axis.text.y = element_text(size = 5))
#where are these centres?????
```
And perhaps this might lead on to mapping...but mapping requires some geocoding of our data, which is an interesting data processing task in itself
```{r echo=TRUE, message=FALSE, warning=FALSE}
#it might be easier to graph with a smaller group of variables
sv1 <- sv %>% select(year, postcode, employment_status, fee_amount, withdrawal_reason, highest_qual, gender, welsh_speaker, natid1, contact_hours, centre_postcode, tutor_id, age, numeric_en)
#read in postcode csv file
uk.postcodes <- read_csv("ukpostcodes.csv")
uk.postcodes <- uk.postcodes %>% select(postcode:longitude)
#this is fine but we have some issues - our postcode column in sv1 is 
#full of lower case letters - this is case sensitive so we need to make these
#proper
sv1$postcode <- str_to_upper(sv1$postcode)
#and merge the info from uk.postcodes onto our sv1 df
sv2 <- sv1 %>% left_join(uk.postcodes, by = "postcode")
#uk.postcodes1 <- uk.postcodes %>% rename(centre_postcode = postcode)
sv3 <- sv2 %>% left_join(uk.postcodes, c("centre_postcode" = "postcode"))
#so long/lats have been read in and  the following determines the distance between points in km
library(geosphere)
sv3$travel_distance <- distHaversine(sv3[,15:16], sv3[,17:18])/1000
#what's our mean/median?
summary(sv3$travel_distance)
#weird outliers
#restrict xlim
sv3 %>% filter(!is.na(travel_distance)) %>% 
  ggplot(aes(travel_distance, fill = numeric_en))+
  geom_histogram()+
  xlim(0,75)+
  ggtitle("Withdrawals and Travel Distance")
```
Perhaps this graph dispels the notion that the greater the travel distance, the more likely someone is to withdraw from the course.
In fact, if a course requires a commitment, of time or money for example, it would appear that people are more inclined to stick at it.

### Unsupervised Machine Learning and Geospatial Analysis

The project will return to the earlier supervised machine learning in the conclusion. It may be of interest to know the likelihood of someone withdrawing from a course but you would still want that person to enrol, so are unlikely to do anything with the information. It may be more beneficial to ask two questions: What is the general make-up of our adult learners? Are there particular groups that are under-represented?
Unsupervised machine learning is not looking to predict anything but is looking for patterns in data. This is powerful and complex. Our complete original dataset has some 8000 rows over 74 columns. If one imagines plotting this data, it might be possible to envisage the multiplicity of the dimensions involved. The following techniques look to reduce that dimensionality and place the data within clusters.
R has a package to deal with a mixed variable dataset using Multiple Correspondence Analysis and Principal Component Analysis...
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(PCAmixdata)
#reduce dataset but keep a mix
sv4pca <- sv1 %>% select(-year, -postcode, -withdrawal_reason, -centre_postcode, -numeric_en)
#split data into two parts - numeric/non-numeric
split <- splitmix(sv4pca)
X1 <- split$X.quanti
X2 <- split$X.quali
#remove columns with 0 variance [important later on in dimension reduction]
X1 <- X1[, sapply(X1, function(v) var(v, na.rm=TRUE)!=0)]
#remove columns with only one level (similar to above)
X2 <- X2[, sapply(X2, function(col) length(unique(col))) > 1]
#PCAmix function - performs PCA on numeric and multiple corresponce analysis on categorical
res.pcamix <- PCAmix(X.quanti = X1, X.quali = X2, rename.level = T, graph = F)
library(reshape2)
melted.loadings <- melt(res.pcamix$sqload)
p <- ggplot(data = melted.loadings, aes(x=Var1, y=Var2, fill=value)) + geom_tile()
p <- p + labs(x = "Variable", y = "Dimension", title = "Variablie influence to principle component")
```
This heatmap representation draws the eye to the lighter shades. Is there some significance to the employment status and the tutor ID? But how many groups or 'clusters' should we split our data into?
```{r echo=TRUE, message=FALSE, warning=FALSE}
p + theme(axis.text.x = element_text(angle = 90, hjust = 1))
set.seed(1)
wss <- 0
for (i in 1:15) {
  km.out <- kmeans(res.pcamix$scores, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}
## K-means Clustering
qplot(1:15, wss,
      xlab = "Number of Clusters",
      ylab = "Within clusters sum of squares",
      main = "Elbow plot of within cluster sum of squares",
      geom = c("point","path"))
# The K-means solution with 4 clusters was chosen as the optimal number of clusters based on the elbow plot.
```
This is known as an elbow plot - the kink in the graph determining the number of clusters to be used. It is not always completely clear how many clusters to use and a number of different options are applicable here. Four clusters were selected initially. What might be the count of the number of people in each cluster?...
```{r echo=TRUE, message=FALSE, warning=FALSE}
kmeans.pca4<-kmeans(res.pcamix$scores, 4, nstart = 20)
X<-kmeans.pca4$cluster
X1<-as.data.frame(cbind(names(X), X))
#define palette for plotting
cpal <- c("#3B7A9E", "#0F8243", "#FF9933", "#D32F2F")
X2 <- X1 %>% group_by(X) %>% count(X)
ggplot(X2, aes(x= X, y = n))+ geom_bar(stat="identity") +
  scale_y_continuous("Count") +
  scale_fill_manual(name = "Cluster", values = cpal) +
  ggtitle("Count within Cluster") +
  xlab("Cluster")
```
and what's the variable level distribution within each cluster across the 5 principal dimensions?...
```{r echo=TRUE, message=FALSE, warning=FALSE}
df2 <- cbind(res.pcamix$scores, kmeans.pca4$cluster)
df2 <- as.data.frame(df2)
colnames(df2) <- c("dim1", "dim2", "dim3", "dim4", "dim5", "4c")
centers.pca4 <- kmeans.pca4$centers
centers.pca4long <- melt(centers.pca4, measure.vars = 1:1)
ggplot(centers.pca4long, aes(x=Var2, y=value, color= as.factor(Var1))) +
  geom_bar(stat = "identity") +
  facet_wrap(~Var1) +
  coord_flip() +
  labs(color = "Cluster") +
  scale_colour_manual(values = cpal) +
  ggtitle("Distribution within Cluster")
```
Now that we have our clusters, we can look at the make up of each in greater detail. For example, what is the age distribution within each cluster? We can also incorporate our binary enrolment key (remembering 1 = enrolled and 0 = withdrawn) into the clusters.
```{r echo=TRUE, message=FALSE, warning=FALSE}
#we need to bind the cluster back onto our original dataset or bind the numeric_en column onto the #cluster dataframe
df.cluster <- cbind(sv4pca, df2$`4c`)
names(df.cluster)[names(df.cluster) == "df2$`4c`"] <- "Cluster"
# subset the data for each cluster
df.cluster1 <- df.cluster[df.cluster$Cluster == 1, ]
df.cluster2 <- df.cluster[df.cluster$Cluster == 2, ]
df.cluster3 <- df.cluster[df.cluster$Cluster == 3, ]
df.cluster4 <- df.cluster[df.cluster$Cluster == 4, ]
cluster.sv <- cbind(df.cluster, sv$numeric_en)
sv.cluster <- cbind(sv, df.cluster$Cluster)
cluster.sv <- rename(cluster.sv, cluster = Cluster)
cluster.sv <- rename(cluster.sv, numeric_en = 'sv$numeric_en')
ggplot(cluster.sv, aes(age, fill = cluster))+
  geom_histogram(binwidth = 10)+
  facet_wrap(~cluster)
```
We can view the enrolment status for each cluster by using grouped summaries...
```{r echo=TRUE, message=FALSE, warning=FALSE}
cluster.sv %>% group_by(cluster, numeric_en) %>% 
  summarise(
    count = n()
  ) %>% mutate(prop = count/sum(count))
```
Further analysis is clearly required to determine just what the make up of our clusters is. What defines them as distinct? We could analyse numerous variables here. The project though, must press on and look at geospatial mapping.
You may recall that the dataset had both individual and centre postcodes enabling us to geo-coordinate these ready for mapping. R can accomodate the popular mapping tool Leaflet, which provides good looking maps with lots of interactivity.
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(leaflet)
sv3 %>% 
  leaflet() %>% 
  addTiles() %>% 
  addMarkers(lng = ~longitude.x, lat = ~latitude.x,
             clusterOptions = markerClusterOptions())
```
So this shows all our adult learners over the 9 years in the area. It might be surprising to note the two distinct outliers in England and Scotland but these are recorded home postcodes - some learners may have been fairly transient. As the map zooms in, the numbered groups split to show where people come from.
We mentioned above in one of our analysis graphs that community learning centres have varying degrees of withdrawal rates. Might we show this on a map?
```{r echo=TRUE, message=FALSE, warning=FALSE}
#can we size the centre on the proportion of withdrawals
sv4 <- sv3 %>% group_by(centre_postcode, longitude.y, latitude.y, numeric_en) %>% 
  summarise(count = n()) %>% mutate(prop = count/sum(count)*100)
#we need to filter to 1 or 0 perhaps
sv4 %>% filter(numeric_en == 1) %>% 
  leaflet() %>% 
  addTiles() %>% 
  addCircles(lng = ~longitude.y, lat = ~latitude.y, radius = ~prop)
```
And here perhaps the addition of colour will aid the visualisation, with red centres having a higher rate of withdrawals. Why these centres in particular?
```{r echo=TRUE, message=FALSE, warning=FALSE}
#can we colour on this proportion perhaps
sv5 <- sv4 %>% filter(numeric_en == 1)
#we can write a function to colour our enrolment proportions
getColor <- function(sv5) {
  sapply(sv5$prop, function(prop) {
    if(prop > 85) {
      "green"
    } else if(prop > 70) {
      "orange"
    } else {
      "red"
    } })
}
centreColour <- getColor(sv5)
sv5 %>% leaflet() %>% addTiles() %>% 
  addCircles(lng = ~longitude.y, lat = ~latitude.y, radius = ~prop, color = centreColour)
```

### Conclusions and Future Work

If we return to the project aims, what have we achieved? There is always likely to be an element of interpretation to analysis based upon human nature. What determines the decisions that people make? Why are we so whimsical? But it is clear that the majority of people that attend adult Welsh language courses are of an older age. Younger people and males are under-represented and any targeted marketing might focus on those two groups. Time constraints seem to be a contributing factor, not just to enrolment but also to course completion. Steps to reduce these time commitments are already in place within the local authority, with courses now offered as part of your working day. Might this be applied in other work environments?
And given a new cohort - what might we be able to determine? Can we predict who is likely to drop out? Well yes, to a degree. By returning to the supervised machine learning and applying the model built from the training dataset we can make predictions on the data we have - how accurate are these predictions?
```{r echo=FALSE, message=FALSE, warning=FALSE}
library(randomForest)
#we've got some NAs which RF doesn't like so we need to select what's predictive and remove NAs
sv4rf <- sv3 %>% select(-year, -postcode, -withdrawal_reason, -centre_postcode, 
                        -latitude.x, -latitude.y, -longitude.x, -longitude.y) %>% na.omit
#train and test datasets at random
smp_size2 <- floor(0.75 * nrow(sv4rf))
set.seed(123)
train_ind2 <- sample(seq_len(nrow(sv4rf)), size = smp_size2)
train.sv4rf <- sv4rf[train_ind2, ]
test.sv4rf <- sv4rf[-train_ind2, ]
#random forest algorithm
rf.train <- train.sv4rf %>% select(highest_qual, contact_hours, age, travel_distance, fee_amount)
rf.label <- train.sv4rf$numeric_en
set.seed(1234)
rf <- randomForest(x = rf.train, y = rf.label, importance = T)
```
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(modelr)
testpredict <- test.sv4rf %>% add_predictions(rf)
empty_col2 <- c()
for (i in 1:length(testpredict$pred)){
  if (testpredict$pred[i] %in% testpredict$numeric_en[i]){
    empty_col2[i] = 1
  }
  else {
    empty_col2[i] = 0
  }
}
testpredict <- cbind(testpredict, empty_col2)
testpredict %>% group_by(empty_col2) %>% summarise(n = n())
```
From this summary table we are accurate 1699 times out of 2056, which gives an 83% accuracy rate. Could this be improved by further engineering of the features or variables that are most predictive? Could this be improved by selecting a more powerful algorithm? Undoubtedly it could.
And what of the secondary aim of the project? Do the PCC employees feel better able to apply the principles of modern data science to the work that they do. Well, yes and no. We have both learnt so much, so quickly from our time on the Accelerator Programme but a bit like learning any language (especially at our ages), if it is not practiced on a day to day basis then we will lose what we have gained. We will continue to bang the drum. Is anyone listening?